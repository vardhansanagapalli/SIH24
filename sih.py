# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AigQnhkq_M7e6UYHCqqRzlkXmoRNbbgb
"""
import os
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.vectorstores.base import VectorStore
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from langchain.chains.qa_with_sources import load_qa_with_sources_chain
from langchain.memory import ConversationBufferMemory

def bot_model(text1):


    AI_TOKEN = "AIzaSyDIotEcNFagyo80lKoDhOyekjR4WCSe-ZY"

    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-pro",
        temperature=0,
        max_tokens=None,
        timeout=None,
        max_retries=2,
        google_api_key = AI_TOKEN
    )

    

    with open('/Users/vardhansans/Downloads/ChatBot-main/Government Museum, Chennai.txt' , 'rt') as file:
        data = ''.join(file.readlines())

    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 120 , chunk_overlap = 30 , separators=["\n\n", "\n", " ", ""])
    doc = text_splitter.split_text(data)

    
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=AI_TOKEN)

    
    persist_directory = '/Users/vardhansans/Downloads/ChatBot-main/Database'

    vectordb = Chroma(
        embedding_function = embeddings,
        persist_directory = persist_directory
    )

    vectordb.add_texts(texts = doc)
    print(vectordb._collection.count())

    

    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-pro",
        temperature=0,
        max_tokens=None,
        timeout=None,
        max_retries=2,
        google_api_key = AI_TOKEN
    )


    # Build prompt
    template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say "thanks for asking!" at the end of the answer.
    {context}
    Question: {question}
    Helpful Answer:"""
    QA_CHAIN_PROMPT = PromptTemplate.from_template(template)

    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True
    )

    qa_chain = load_qa_with_sources_chain(
        llm=llm,
        prompt=QA_CHAIN_PROMPT
    )

    qa_chain = ConversationalRetrievalChain.from_llm(
        llm,
        retriever=vectordb.as_retriever(),
        memory = memory,
        return_source_documents=True,
        chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
    )

    
    question = text1
    print(f"Me : {question}")
    result = qa_chain({"question" : question})
    print(f"Bot : {result}")
    return result["result"]

print(bot_model("What are the timings?"))